{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "CodigHoaxWithML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## Done"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive; drive.mount(\"/content/drive\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "eiqui6lsDkie",
        "outputId": "42dbbd84-195f-45cf-b134-9b7f2298a45f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk \r\n",
        "import re\r\n",
        "nltk.download(\"stopwords\");nltk.download(\"punkt\")\r\n",
        "import sklearn "
      ],
      "outputs": [],
      "metadata": {
        "id": "XUPz3ro6FLZB",
        "outputId": "efc7ed79-9ff3-4127-9a65-956c238d78d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data1 = pd.read_excel(\"/content/drive/My Drive/DimasASu/Data Latih BDC.xlsx\")\r\n",
        "data2 = pd.read_ csv(\"/content/drive/My Drive/DimasASu/data_clean.csv\")\r\n",
        "\r\n",
        "dataTest = pd.read_csv(\"/content/drive/My Drive/DimasASu/datatest_labelled.csv\")\r\n",
        "\r\n",
        "data1.head() , data2.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "BXQwcKNwFBBU",
        "outputId": "bfb08b0c-8fbf-4aff-fcc5-ba96a852270b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data1 = data1[[\"judul\",\"narasi\",\"label\"]]\r\n",
        "data2 = data2[[\"judul\",\"narasi\",\"label\"]]\r\n",
        "\r\n",
        "df_train = pd.concat([data1,data2])\r\n",
        "df_train = df_train.reset_index().drop(\"index\",axis = 1)\r\n",
        "df_train.tail()"
      ],
      "outputs": [],
      "metadata": {
        "id": "M0Me_7j2FOrL",
        "outputId": "0cb3d251-bae1-4762-c9a1-b7d8cf3a24d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def clean(text_messages):\r\n",
        "    text_messages = str(text_messages)\r\n",
        "    processed = re.sub(r'^.+@[^\\.].*\\.[a-z]{2,}$',\r\n",
        "                                    'alamat email',text_messages)\r\n",
        "\r\n",
        "    # Replace URLs with 'webaddress'\r\n",
        "    processed = re.sub(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\r\n",
        "                                      'alamat web',processed)\r\n",
        "\r\n",
        "    # Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\r\n",
        "    processed = re.sub(r'£|\\$', 'moneysymb',processed)\r\n",
        "        \r\n",
        "    # Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\r\n",
        "    processed = re.sub(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\r\n",
        "                                      'nomor hp',processed)\r\n",
        "        \r\n",
        "    # Replace numbers with 'numbr'\r\n",
        "    processed = re.sub(r'\\d+(\\.\\d+)?', 'numbr',processed)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    # Remove punctuation\r\n",
        "    processed = re.sub(r'[^\\w\\d\\s]', ' ',processed)\r\n",
        "\r\n",
        "    # Replace whitespace between terms with a single space\r\n",
        "    processed = re.sub(r'\\s+', ' ',processed)\r\n",
        "\r\n",
        "    # Remove leading and trailing whitespace\r\n",
        "    processed = re.sub(r'^\\s+|\\s+?$', '',processed)\r\n",
        "\r\n",
        "    processed = processed.lower()\r\n",
        "\r\n",
        "    return processed"
      ],
      "outputs": [],
      "metadata": {
        "id": "U6q843tHFa-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_train[\"narasi\"] = df_train[\"narasi\"].apply(clean)\n",
        "df_train[\"judul\"] = df_train[\"judul\"].apply(clean)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yv6RlEUXWAXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwordsObj = set(stopwords.words(\"indonesian\"))\n",
        "\n",
        "df_train[\"narasi\"] = df_train[\"narasi\"].apply(lambda sentence: \" \".join(word for word in sentence.split(\" \") if word not in stopwordsObj))\n",
        "df_train[\"judul\"] = df_train[\"judul\"].apply(lambda sentence: \" \".join(word for word in sentence.split(\" \") if word not in stopwordsObj))"
      ],
      "outputs": [],
      "metadata": {
        "id": "arUxPJ4sVzS6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "porterStemmer = nltk.PorterStemmer()\n",
        "df_train[\"narasi\"] = df_train[\"narasi\"].apply(lambda sentence: \" \".join(porterStemmer.stem(word) for word in sentence.split(\" \")) )\n",
        "\n",
        "df_train[\"narasi\"].head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "h-AXWizVXoGX",
        "outputId": "e4f35c80-5262-4379-ecf6-a8dbd90c0a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for message in df_train[\"narasi\"]:\n",
        "  words = word_tokenize(message)\n",
        "  for word in words:\n",
        "    corpus.append(word)\n",
        "\n",
        "for message in df_train[\"judul\"]:\n",
        "  words = word_tokenize(message)\n",
        "  for word in words:\n",
        "    corpus.append(word)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BwUEuEkZXuOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "corpus = nltk.FreqDist(corpus)\n",
        "corpus"
      ],
      "outputs": [],
      "metadata": {
        "id": "X3RSolBAX3Hn",
        "outputId": "eae56469-25c6-4a6f-bc2e-67732b178df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "word_features = list(corpus.keys())[:5000]"
      ],
      "outputs": [],
      "metadata": {
        "id": "dZvkyP-OYIob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def find_features(message):\n",
        "  message = str(message)\n",
        "  words = word_tokenize(message)\n",
        "  features = {}\n",
        "  for word in word_features:\n",
        "    features[word] = word in words\n",
        "  \n",
        "  return features"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fg3p468QYLFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "messages = list(zip(df_train[\"narasi\"],df_train[\"label\"]))\n",
        "np.random.seed(1)\n",
        "print(messages)\n",
        "featuresets = [(find_features(text),label) for (text,label) in messages] "
      ],
      "outputs": [],
      "metadata": {
        "id": "sekItfY-YPeK",
        "outputId": "da7b29f6-9312-4555-c924-4370f1e13172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "featuresets[:5]"
      ],
      "outputs": [],
      "metadata": {
        "id": "A7FYI1CKafJq",
        "outputId": "ad013545-ddfb-43fa-a43b-939b11677682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "training,testing = train_test_split(featuresets,test_size = 0.05,random_state = 1)\n",
        "\n",
        "print(len(training), len(testing))"
      ],
      "outputs": [],
      "metadata": {
        "id": "3WMpvW7DYoX9",
        "outputId": "0e66468c-84ab-45e7-b211-9d2742d7ae47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk import SklearnClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Define models to train\n",
        "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
        "         \"Naive Bayes\", \"SVM Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression(),\n",
        "    SGDClassifier(max_iter = 100),\n",
        "    MultinomialNB(),\n",
        "    SVC(kernel = 'linear')\n",
        "]\n",
        "\n",
        "models = zip(names, classifiers)\n",
        "\n",
        "for name, model in models:\n",
        "    nltk_model = SklearnClassifier(model)\n",
        "    nltk_model.train(training)\n",
        "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
        "    print(\"{} Accuracy: {}\".format(name, accuracy))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ESb-NkDjaNam",
        "outputId": "a0a3f2c0-83ca-4e41-ac7d-be7b9545e375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
        "         \"Naive Bayes\", \"SVM Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression(),\n",
        "    SGDClassifier(max_iter = 100),\n",
        "    MultinomialNB(),\n",
        "    SVC(kernel = 'linear')\n",
        "]\n",
        "\n",
        "models = list(zip(names, classifiers))\n",
        "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
        "nltk_ensemble.train(training)\n",
        "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
        "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
      ],
      "outputs": [],
      "metadata": {
        "id": "tDt4G10whI8v",
        "outputId": "9efad7cd-1c58-4d59-9162-2995455b91cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    }
  ]
}